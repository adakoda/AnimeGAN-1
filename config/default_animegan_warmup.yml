mode: train
model:
  name: animegan
  helper: AnimeGanHelper
  helper_kwarg:
    dataset_root: /home/zqh/workspace/animedataset
    in_hw:
      - 256
      - 256
    mixed_precision_dtype: float32
    hparams:
      style: Hayao # use which anime style for training
  network: animenet
  network_kwarg:
    image_shape:
      - null
      - null
      - 3
    filters: 64
    nblocks: 3
    use_sn: true
train:
  graph_optimizer: true
  graph_optimizer_kwarg:
    layout_optimizer: true
    remapping: true
    shape_optimization: true
    loop_optimization: true
    constant_folding: true
    function_optimization: true
    scoped_allocator_optimization: true
  distributionstrategy_kwarg:
    tpu: null
    strategy: Mirrored
  augmenter: true
  batch_size: 2
  pre_ckpt: null
  rand_seed: 10101
  epochs: 1
  train_epoch_step: null
  vali_epoch_step: null
  steps_per_run: 30
  log_dir: log
  sub_log_dir: default_animegan_warmup_exp
  mixed_precision:
    enable: false
    dtype: mixed_float16
  trainloop: AnimeGanInitLoop
  trainloop_kwarg:
    hparams:
      wc: 1.5 # l1 loss with pre-trained model weight
      ema:
        enable: false
        decay: 0.999
  variablecheckpoint_kwarg:
    variable_dict:
      g_model: generator_model
      d_model: discriminator_model
      g_optimizer: generator_optimizer
      d_optimizer: discriminator_optimizer
    monitor: train/g_loss
    mode: min
  generator_optimizer: Adam
  generator_optimizer_kwarg:
    learning_rate: 0.0001 # 0.00016
    beta_1: 0.5
    beta_2: 0.999
  discriminator_optimizer: Adam
  discriminator_optimizer_kwarg:
    learning_rate: 0.00008
    beta_1: 0.5
    beta_2: 0.999
  callbacks:
    - name: EarlyStopping
      kwarg:
        monitor: train/g_loss
        min_delta: 0
        patience: 20
        verbose: 0
        mode: min
        baseline: null
        restore_best_weights: false
    # - name: ScheduleLR
    #   kwarg:
    #     base_lr: 0.03
    #     use_warmup: true
    #     warmup_epochs: 20
    #     decay_rate: 0.7
    #     decay_epochs: 20
    #     outside_optimizer: generator_optimizer
    # - name: ScheduleLR
    #   kwarg:
    #     base_lr: 0.03
    #     use_warmup: true
    #     warmup_epochs: 20
    #     decay_rate: 0.7
    #     decay_epochs: 20
    #     outside_optimizer: discriminator_optimizer
